{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "#import collections\n",
    "\n",
    "train_bool=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def obtain_data(train_bool=True):\n",
    "    if train_bool:\n",
    "        volume_filename = \"volume_train.csv\"\n",
    "    else:\n",
    "        volume_filename = \"volume_test.csv\"\n",
    "\n",
    "    with open(volume_filename) as csvfile:\n",
    "        rows = [row for row in csv.DictReader(csvfile)]\n",
    "\n",
    "    fields = set([k for (k,_) in rows[0].items()])\n",
    "    print(\"Fields:\", fields)\n",
    "\n",
    "    with open(\"raw_2010.csv\") as csvfile:\n",
    "        raw_rows = [row for row in csv.DictReader(csvfile)]\n",
    "    with open(\"raw_2011.csv\") as csvfile:\n",
    "        raw_rows += [row for row in csv.DictReader(csvfile)]\n",
    "    \n",
    "    return rows, raw_rows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pvalues = {}\n",
    "# key: station #\n",
    "# value: p value\n",
    "def obtain_pvalues(exp_num=2, train_bool=True):\n",
    "    pv_fnames = [\"ExtremeTempTraffic\",\"TempTraffic\",\"WeatherTraffic\",\"WeekendWeekday\"]\n",
    "    fname = pv_fnames[exp_num]\n",
    "\n",
    "    if train_bool:\n",
    "        fname += \"_Train\"\n",
    "    else:\n",
    "        fname += \"_Test\"\n",
    "    \n",
    "    filename = os.path.join(\"pvalues\", fname +\".csv\")\n",
    "    with open(filename) as csvfile:\n",
    "        pvalues = {float(row[\"stations\"]): float(row[\"station.p.vals\"]) for row in csv.DictReader(csvfile)}\n",
    "    \n",
    "    return pvalues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields: {'', 'daily.data.apparentTemperatureHigh', 'daily.data.precipType', 'daily.data.temperatureLow', 'Date', 'Month', 'daily.data.windSpeed', 'Season', 'Month_y', 'daily.data.icon', 'Month_x', 'daily.data.apparentTemperatureLow', 'daily.data.precipIntensity', 'Outflow', 'Total', 'Inflow', 'daily.data.temperatureHigh', 'X', 'Precip_bin', 'Wday_bin', 'Wday', 'daily.data.humidity', 'daily.data.precipProbability', 'daily.data.summary', 'Hday_bin', 'Station'}\n"
     ]
    }
   ],
   "source": [
    "rows, raw_rows = obtain_data(train_bool=train_bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For myself and my project partners\n",
    "\n",
    "# SDR: Station Date Rides\n",
    "\n",
    "# key: (station, date)\n",
    "# value: list of indices of raw data \n",
    "def obtain_sdr(raw_rows):\n",
    "    station_date_rides = {}\n",
    "    for i,row in enumerate(raw_rows):\n",
    "        date = row[\"Start date\"].split(\" \")[0]\n",
    "\n",
    "        station = row[\"Start station number\"]\n",
    "        if not (station, date) in station_date_rides.keys():\n",
    "            station_date_rides[(station, date)] = []\n",
    "        station_date_rides[(station, date)].append(i)\n",
    "\n",
    "        station = row[\"End station number\"]\n",
    "        if not (station, date) in station_date_rides.keys():\n",
    "            station_date_rides[(station, date)] = []\n",
    "        station_date_rides[(station, date)].append(i)\n",
    "    return station_date_rides\n",
    "\n",
    "# Output to CSV with 1-indexing, for correct use in R\n",
    "def output_sdr_csv():\n",
    "    station_date_rides = obtain_sdr()\n",
    "    \n",
    "    # use i+1 so that indexing is as in R\n",
    "    for k, ilist in station_date_rides.items():\n",
    "        ilist = [i+1 for i in ilist]\n",
    "\n",
    "    with open(\"station_date_trips.csv\", \"w+\") as outfile:\n",
    "        #wrt = csv.writer(outfile)# quoting=csv.QUOTE_NONE)\n",
    "        wrt = csv.writer(outfile, quoting=csv.QUOTE_ALL)\n",
    "        wrt.writerow([\"Station\", \"Date\", \"Trips\"])\n",
    "        for k,val in station_date_rides.items():\n",
    "            station, date = k\n",
    "            third = \" \".join(tuple(str(v) for v in val))\n",
    "            #wrt.writerow([station, date, '\"' + third + '\"'])\n",
    "            wrt.writerow([station, date,third])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# key: station\n",
    "# value: dict of daily traffic values with: \n",
    "#   (key: date, val: traffic level value)\n",
    "def obtain_station_days_traffic(rows):\n",
    "    station_days = {}\n",
    "    for row in rows:\n",
    "        station = row[\"Station\"]\n",
    "        if not station in station_days.keys():\n",
    "            station_days[station] = {}\n",
    "        date = row[\"Date\"]\n",
    "        station_days[station][date] = row[\"X\"]\n",
    "    return station_days\n",
    "    \n",
    "# Return indices of those discoveries made\n",
    "def bh(pv_dict, alpha=.05, storey=False, printing=False):\n",
    "    if storey:\n",
    "        alpha /= storey_pihat(pvs)\n",
    "        \n",
    "    n = float(len(pv_dict))\n",
    "    k = int(n) -1\n",
    "    pvs = [(p,i) for i,p in enumerate(pv_dict)]\n",
    "    pvs.sort()\n",
    "    while not pvs[k-1][0] <= (alpha*k)/n:\n",
    "        if printing:\n",
    "            print(pvs[k-1][0], (alpha*k)/n)\n",
    "        k -= 1\n",
    "        if k == 0:\n",
    "            break\n",
    "    return [pv[1] for pv in pvs[:k]]\n",
    "\n",
    "#def storey_bh(pvs, alpha, gamma=0.5):\n",
    "def storey_pihat(pv_dict, gamma=0.5):\n",
    "    num = 0.\n",
    "    for _, pv in pvs.items():\n",
    "        if pv > gamma:\n",
    "            num += 1.\n",
    "    denom = len(pvs) * (1.-gamma)\n",
    "    #if denom:\n",
    "    pihat = min(num/denom, 1)\n",
    "    return pihat\n",
    "    \n",
    "#  function\n",
    "def group_adaptive_bh(pv_dict, groups_indices, alpha=.05, gamma=0.5):    \n",
    "    groups = [[p_vector[i] for i in gi] for gi in groups_indices]\n",
    "    # Pi hat for each group\n",
    "    group_pi_hats = [storey_pihat(group) for group in groups]\n",
    "\n",
    "    for i,group in enumerate(groups_indices):\n",
    "        for j in group:\n",
    "            p_vector[j] *= group_pi_hats[i]\n",
    "            \n",
    "    for i,p in enumerate(p_vector):\n",
    "        if p > gamma:\n",
    "            p_vector[i] = np.infty\n",
    "\n",
    "    # Now do ordinary BH on mod_p_vector\n",
    "    return bh(p_vector, alpha)\n",
    "\n",
    "def old_group_adaptive_bh(p_vector, groups_indices, alpha=.05, gamma=0.5):    \n",
    "    groups = [[p_vector[i] for i in gi] for gi in groups_indices]\n",
    "    # Pi hat for each group\n",
    "    group_pi_hats = [storey_pihat(group) for group in groups]\n",
    "\n",
    "    for i,group in enumerate(groups_indices):\n",
    "        for j in group:\n",
    "            p_vector[j] *= group_pi_hats[i]\n",
    "            \n",
    "    for i,p in enumerate(p_vector):\n",
    "        if p > gamma:\n",
    "            p_vector[i] = np.infty\n",
    "\n",
    "    # Now do ordinary BH on mod_p_vector\n",
    "    return bh(p_vector, alpha)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_days contains train vs test information\n",
    "# tells us which subset of raw_rows to use\n",
    "#def obtain_raw_subset(raw_rows, station_days)\n",
    "\n",
    "\n",
    "def obtain_duration_groups(rows, raw_rows, station_date_rides):\n",
    "    # key: station\n",
    "    # value: (total duration, number of trips)\n",
    "    # then can compute avg by taking v[0]/v[1]\n",
    "    station_durations = {}\n",
    "    for row in rows:\n",
    "        station = row[\"Station\"]\n",
    "        date = row[\"Date\"]    \n",
    "        if (station, date) in station_date_rides:\n",
    "            \n",
    "            if not station in station_durations.keys():\n",
    "                station_durations[station] = [0,0]\n",
    "\n",
    "            total_duration = 0\n",
    "            \n",
    "            for i in station_date_rides[(station, date)]:\n",
    "                total_duration += float(raw_rows[i][\"Duration\"])\n",
    "\n",
    "            station_durations[station][0] += total_duration\n",
    "            station_durations[station][1] += len(station_date_rides[(station, date)])\n",
    "    \n",
    "    sort_station_durations = [(v[0]/v[1],k) for k,v in station_durations.items()]\n",
    "    sort_station_durations.sort()\n",
    "    #sort_station_durations_names = sort_station_durations\n",
    "    sort_station_durations_names = [k for (_,k) in sort_station_durations]\n",
    "    \n",
    "    nstations = len(sort_station_durations_names)\n",
    "    ngroups = 4\n",
    "    ratio = int(nstations/ngroups)\n",
    "    \n",
    "    groups = []\n",
    "    for i in range(ngroups):\n",
    "        groups.append(sort_station_durations_names[ratio*i:ratio*(i+1)])\n",
    "    # fill the last group with stragglers, if rounding error on indices\n",
    "    groups[-1] += sort_station_durations_names[ratio*ngroups:]\n",
    "    return groups\n",
    "\n",
    "def obtain_membership_groups(rows, raw_rows, station_date_rides):\n",
    "    # key: station\n",
    "    # value: (number of member trips, total number of trips)\n",
    "    # then can compute rate of membership by taking v[0]/v[1]\n",
    "    station_rates = {}\n",
    "    for row in rows:\n",
    "        station = row[\"Station\"]\n",
    "        date = row[\"Date\"]    \n",
    "        if (station, date) in station_date_rides:\n",
    "            \n",
    "            if not station in station_rates.keys():\n",
    "                station_rates[station] = [0,0]\n",
    "\n",
    "        \n",
    "            member_rides = 0\n",
    "            \n",
    "            for i in station_date_rides[(station, date)]:\n",
    "                member_rides += (raw_rows[i][\"Member type\"] == \"Member\")\n",
    "\n",
    "            station_rates[station][0] += member_rides\n",
    "            station_rates[station][1] += len(station_date_rides[(station, date)])\n",
    "        \n",
    "    \n",
    "    sort_station_durations = [(v[0]/v[1],k) for k,v in station_rates.items()]\n",
    "    sort_station_durations.sort()\n",
    "    #sort_station_durations_names = sort_station_durations\n",
    "    sort_station_durations_names = [k for (_,k) in sort_station_durations]\n",
    "    \n",
    "    nstations = len(sort_station_durations_names)\n",
    "    ngroups = 4\n",
    "    ratio = int(nstations/ngroups)\n",
    "    \n",
    "    groups = []\n",
    "    for i in range(ngroups):\n",
    "        groups.append(sort_station_durations_names[ratio*i:ratio*(i+1)])\n",
    "    # fill the last group with stragglers, if rounding error on indices\n",
    "    groups[-1] += sort_station_durations_names[ratio*ngroups:]\n",
    "    return groups\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_date_rides = obtain_sdr(raw_rows)\n",
    "#groups = obtain_duration_groups(rows, raw_rows, station_date_rides)\n",
    "#groups = obtain_membership_groups(rows, raw_rows, station_date_rides)\n",
    "\n",
    "\n",
    "#for group in groups:\n",
    "    #print((group))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (station, pvalue) tuples\n",
    "def index_groups(groups, pvalues):\n",
    "    groups = [[int(g) for g in group] for group in groups]\n",
    "    return groups\n",
    "    igroups = []\n",
    "    for group in groups:\n",
    "        igroup = []\n",
    "        for i in range(len(pvalues)):\n",
    "            if pvalues[i][0] in group:\n",
    "                igroup.append(i)\n",
    "        igroups.append(igroup)\n",
    "    return igroups\n",
    "\n",
    "def execute_all(rows, raw_rows, station_date_rides, train_bool=True):\n",
    "    alpha=.05\n",
    "    \n",
    "    duration_groups = obtain_duration_groups(rows, raw_rows, station_date_rides)\n",
    "    \n",
    "    membership_groups = obtain_membership_groups(rows, raw_rows, station_date_rides)\n",
    "\n",
    "    results = []\n",
    "    pvaluess = []\n",
    "    for exp_num in range(4):\n",
    "        pvalues = list(obtain_pvalues(exp_num=exp_num, train_bool=train_bool).items())\n",
    "        pvaluess.append(pvalues)\n",
    "        #plist = [p for (_,p) in pvalues.items()]\n",
    "        plist = [p for (_,p) in pvalues]\n",
    "        \n",
    "        duration_igroups = index_groups(duration_groups, pvalues)\n",
    "        membership_igroups = index_groups(membership_groups, pvalues)\n",
    "\n",
    "        result = []\n",
    "        result.append(bh(plist, alpha=alpha, storey=False))\n",
    "        result.append(bh(plist, alpha=alpha, storey=True))\n",
    "        result.append(group_adaptive_bh(plist, duration_igroups, alpha=alpha))\n",
    "        result.append(group_adaptive_bh(plist, membership_igroups, alpha=alpha))\n",
    "        results.append(result)\n",
    "        \n",
    "    return results, pvaluess\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, pvaluess = execute_all(rows, raw_rows, station_date_rides, train_bool=train_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min, max: 0 64\n",
      "[0, 2, 2, 64]\n",
      "min, max: 106 119\n",
      "[106, 109, 112, 119]\n",
      "min, max: 0 1\n",
      "[0, 0, 0, 1]\n",
      "min, max: 14 14\n",
      "[14, 14, 14, 14]\n",
      "[[[], [26, 113], [113, 26], [113, 26, 0, 37, 95, 13, 51, 76, 88, 44, 103, 99, 97, 46, 56, 91, 11, 61, 136, 25, 45, 101, 104, 72, 65, 71, 75, 66, 38, 82, 139, 40, 12, 60, 55, 80, 90, 81, 33, 86, 17, 68, 87, 58, 70, 79, 14, 102, 93, 30, 35, 52, 1, 92, 24, 77, 83, 57, 32, 100, 122, 41, 53, 43]], [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 25, 27, 28, 30, 31, 32, 33, 34, 37, 38, 39, 40, 43, 44, 45, 46, 47, 48, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 102, 105, 29, 41, 50, 54, 94, 103, 42, 49, 75, 24, 64, 22, 79, 36, 113, 104, 26, 35, 114], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 25, 27, 28, 30, 31, 32, 33, 34, 37, 38, 39, 40, 43, 44, 45, 46, 47, 48, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 102, 105, 29, 41, 50, 54, 94, 103, 42, 49, 75, 24, 64, 22, 79, 36, 113, 104, 26, 35, 114, 63, 110, 109], [9, 12, 17, 18, 23, 25, 33, 37, 38, 40, 52, 57, 61, 68, 70, 81, 83, 84, 86, 88, 90, 95, 97, 99, 100, 102, 94, 103, 0, 1, 2, 4, 6, 7, 8, 11, 13, 14, 15, 16, 19, 20, 21, 28, 31, 32, 39, 43, 44, 45, 46, 47, 48, 51, 53, 56, 58, 60, 65, 66, 67, 69, 72, 74, 77, 78, 80, 82, 87, 89, 91, 92, 93, 105, 79, 29, 41, 50, 113, 42, 75, 3, 5, 10, 27, 30, 34, 55, 59, 62, 71, 73, 76, 85, 96, 98, 104, 24, 54, 49, 64, 22, 26, 36, 35, 114, 63, 109, 119, 122, 136, 141], [0, 7, 9, 11, 12, 13, 15, 17, 18, 23, 25, 26, 28, 31, 32, 33, 35, 37, 38, 40, 41, 43, 44, 45, 46, 48, 51, 52, 56, 57, 58, 61, 65, 66, 67, 68, 70, 75, 76, 77, 79, 80, 81, 82, 83, 84, 86, 87, 88, 90, 91, 92, 93, 95, 97, 99, 100, 101, 102, 103, 104, 105, 113, 122, 123, 136, 137, 139, 94, 1, 2, 4, 6, 8, 14, 16, 19, 21, 39, 47, 53, 69, 72, 78, 50, 42, 3, 10, 27, 55, 62, 71, 73, 96, 98, 24, 54, 20, 60, 74, 89, 29, 36, 5, 30, 34, 59, 85, 63, 109, 49, 64, 22, 114, 108, 119, 141, 110, 129]], [[], [], [], [102]], [[32, 35, 39, 59, 73, 96, 119, 2, 24, 48, 56, 19, 65, 114], [32, 35, 39, 59, 73, 96, 119, 2, 24, 48, 56, 19, 65, 114], [35, 32, 39, 59, 73, 96, 119, 48, 56, 2, 24, 65, 114, 19], [35, 32, 39, 59, 73, 96, 119, 56, 48, 2, 24, 65, 114, 19]]]\n"
     ]
    }
   ],
   "source": [
    "def analyze_results(results):\n",
    "    for result in results:\n",
    "        rlens = [len(r) for r in result]\n",
    "        minn = min(rlens)\n",
    "        maxx = max(rlens)\n",
    "        print(\"min, max:\", minn, maxx)\n",
    "        print(rlens)\n",
    "\n",
    "analyze_results(results)\n",
    "print(results)\n",
    "\n",
    "\n",
    "\n",
    "#print(min([4,5,2,3]))\n",
    "\n",
    "if False:\n",
    "    #station_days_traffic = obtain_station_days_traffic(rows)\n",
    "    print(station_date_rides[('31200', '2011-02-07')])\n",
    "\n",
    "    for row in raw_rows:\n",
    "        if row[\"Start station number\"] == \"31404\": #and row[\"Date\"][:10] == '2011-02-07':\n",
    "            print(row)    \n",
    "        if row[\"End station number\"] == \"31404\": #and row[\"Date\"][:10] == '2011-02-07':\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for row in rows:\n",
    "        if row[\"Station\"] == \"31404\": #and row[\"Date\"][:10] == '2011-02-07':\n",
    "            print(row)    \n",
    "\n",
    "    #station_dates = [(station,day)  for (station,dct) in station_days_traffic.items() for day in dct.keys()]\n",
    "    #print( ( station_dates ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_loc_data(stations, rejections):\n",
    "    \n",
    "    with open(\"location_data.csv\") as csvfile:\n",
    "        rows = [row for row in csv.DictReader(csvfile)]\n",
    "    \n",
    "    results = []\n",
    "    for row in rows:\n",
    "        rejection = rejections[row[\"Station\"]]\n",
    "        result = [row[\"Latitude\"], row[\"Longitude\"], str(rejection)] \n",
    "    \n",
    "    with open(\"loc_coords.csv\", \"w+\") as outfile:\n",
    "        wrt = csv.writer(outfile, quoting=csv.QUOTE_NONE)\n",
    "        wrt.writerow([\"Lat\", \"Lon\", \"Rejected\"])\n",
    "        for res in result:\n",
    "            wrt.writerow(res)\n",
    "#parse_loc_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
