{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def obtain_data(train_bool=True):\n",
    "    if train_bool:\n",
    "        volume_filename = \"volume_train.csv\"\n",
    "    else:\n",
    "        volume_filename = \"volume_test.csv\"\n",
    "\n",
    "    with open(volume_filename) as csvfile:\n",
    "        #rows = [row for row in csv.reader(csvfile)]\n",
    "        rows = [row for row in csv.DictReader(csvfile)]\n",
    "\n",
    "    fields = set([k for (k,_) in rows[0].items()])\n",
    "    print(fields)\n",
    "\n",
    "    with open(\"raw_2010.csv\") as csvfile:\n",
    "        raw_rows = [row for row in csv.DictReader(csvfile)]\n",
    "    with open(\"raw_2011.csv\") as csvfile:\n",
    "        raw_rows += [row for row in csv.DictReader(csvfile)]\n",
    "    \n",
    "    return rows, raw_rows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pvalues = {}\n",
    "# key: station #\n",
    "# value: p value\n",
    "def obtain_pvalues(exp_num=2, train_bool=True)\n",
    "    pv_fnames = [\"ExtremeTempTraffic\",\"TempTraffic\",\"WeatherTraffic\",\"WeekendWeekday\"]\n",
    "    fname = pv_fnames[exp_num]\n",
    "\n",
    "    if train_bool:\n",
    "        fname += \"_Train.csv\"\n",
    "    else:\n",
    "        fname += \"_Test.csv\"\n",
    "\n",
    "    with open(fname) as csvfile:\n",
    "        pvalues = {row[\"stations\"]: row[\"station.p.vals\"] for row in csv.DictReader(csvfile)}\n",
    "    \n",
    "    return pvalues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, raw_rows = obtain_data()\n",
    "pvalues = obtain_pvalues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For myself and my project partners\n",
    "\n",
    "# SDR: Station Day Rides\n",
    "\n",
    "# key: (station, date)\n",
    "# value: list of indices of raw data \n",
    "def obtain_sdr():\n",
    "    station_date_rides = {}\n",
    "    for i,row in enumerate(raw_rows):\n",
    "        date = row[\"Start date\"].split(\" \")[0]\n",
    "\n",
    "        station = row[\"Start station number\"]\n",
    "        if not (station, date) in station_date_rides.keys():\n",
    "            station_date_rides[(station, date)] = []\n",
    "        station_date_rides[(station, date)].append(i)\n",
    "\n",
    "        station = row[\"End station number\"]\n",
    "        if not (station, date) in station_date_rides.keys():\n",
    "            station_date_rides[(station, date)] = []\n",
    "        station_date_rides[(station, date)].append(i)\n",
    "    return station_date_rides\n",
    "\n",
    "# Output to CSV with 1-indexing, for correct use in R\n",
    "def output_sdr_csv():\n",
    "    station_date_rides = obtain_sdr()\n",
    "    \n",
    "    # use i+1 so that indexing is as in R\n",
    "    for k, ilist in station_date_rides.items():\n",
    "        ilist = [i+1 for i in ilist]\n",
    "\n",
    "    with open(\"station_date_trips.csv\", \"w+\") as outfile:\n",
    "        #wrt = csv.writer(outfile)# quoting=csv.QUOTE_NONE)\n",
    "        wrt = csv.writer(outfile, quoting=csv.QUOTE_ALL)\n",
    "        wrt.writerow([\"Station\", \"Date\", \"Trips\"])\n",
    "        for k,val in station_date_rides.items():\n",
    "            station, date = k\n",
    "            third = \" \".join(tuple(str(v) for v in val))\n",
    "            #wrt.writerow([station, date, '\"' + third + '\"'])\n",
    "            wrt.writerow([station, date,third])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Station'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cf13300ed75b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstation_date_rides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mstation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Station\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Start date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstation_date_rides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Station'"
     ]
    }
   ],
   "source": [
    "\n",
    "# key: station\n",
    "# value: dict of daily traffic values with: \n",
    "#   (key: date, val: traffic level value)\n",
    "def obtain_station_days(rows):\n",
    "    station_days = {}\n",
    "    for row in rows:\n",
    "        station = row[\"Station\"]\n",
    "        if not station in station_days.keys():\n",
    "            station_days[station] = {}\n",
    "        date = row[\"Date\"]\n",
    "        station_days[station][date] = row[\"X\"]\n",
    "    return station_days\n",
    "    \n",
    "# Return indices of those discoveries made\n",
    "def bh(pvs, alpha):\n",
    "    k = 0\n",
    "    n = float(len(pvs))\n",
    "    pvs = [(p,i) for i,p in enumerate(pvs)]\n",
    "    pvs.sort()\n",
    "    while pvs[k][0] <= (alpha*k)/n:\n",
    "        k += 1\n",
    "    return [pv[1] for pv in pvs[:k]]\n",
    "\n",
    "#def storey_bh(pvs, alpha, gamma=0.5):\n",
    "def storey_pihat(group, gamma=0.5):\n",
    "    num = 0.\n",
    "    for pv in group:\n",
    "        if pv > gamma:\n",
    "            num += 1.\n",
    "    denom = len(group) * (1.-gamma)\n",
    "    pihat = min(num/denom, 1)\n",
    "    \n",
    "from copy import deepcopy\n",
    "#  function\n",
    "def group_adaptive_bh(p_vector, groups_indices, alpha, gamma=0.5):\n",
    "    #groups = []\n",
    "    #n = 0\n",
    "    #for group_size in group_sizes:\n",
    "    #    group = p_vector[n: n + group_size]\n",
    "    #    groups.append(group)\n",
    "    #    n += group_size\n",
    "    \n",
    "    groups = [[p_vector[i] for i in gi] for gi in groups_indices]\n",
    "\n",
    "    # Pi hat for each group\n",
    "    group_pi_hats = []\n",
    "    for gindex, group in enumerate(groups):\n",
    "        num = 0.\n",
    "        for pv in group:\n",
    "            if pv > gamma:\n",
    "                num += 1.\n",
    "        denom = len(group) * (1.-gamma)\n",
    "        group_pi_hats.append(min(num/denom, 1))\n",
    "\n",
    "    for i,group in enumerate(groups):\n",
    "        for j in group:\n",
    "            p_vector[j] *= group_pi_hat[i]\n",
    "            \n",
    "    # bogus approach: cannot iterate correct : g_hat_vector = [p * group_pi_hats[ [i]] for i,p in enumerate(p_vector)]\n",
    "    #for group, pi_hat in zip(groups, group_pi_hats):\n",
    "    #    g_hat_vector += [pi_hat for _ in range(len(group))]\n",
    "    #assert len(g_hat_vector) == len(p_vector)\n",
    "    #mod_p_vector = [g_hat * p for g_hat,p in zip(g_hat_vector, p_vector)]\n",
    "    for i,p in enumerate(p_vector):\n",
    "        if p > gamma:\n",
    "            p_vector[i] = np.infty\n",
    "\n",
    "    # Get modified weighted p values\n",
    "    # Now do ordinary BH on mod_p_vector\n",
    "    return bh(mod_p_vector, alpha)\n",
    "    #sorted_p_vector = sorted(mod_p_vector)\n",
    "    #for k in range(len(sorted_p_vector)-1, 1, -1):\n",
    "    #    bound = (alpha*k)/n\n",
    "    #    if sorted_p_vector[k][0] < bound:\n",
    "    #        return k, sorted_p_vector\n",
    "    #return len(sorted_p_vector)-1, sorted_p_vector\n",
    "    # reject 1..k    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key: station\n",
    "# value: list of dates of days of which station is valid within train/test/validate\n",
    "valid_station_days = {}\n",
    "for row in rows:\n",
    "    station = row[\"Station\"]\n",
    "    date = row[\"Date\"]\n",
    "    if not station in valid_station_days.keys():\n",
    "        valid_station_days[station] = []\n",
    "    valid_station_days[station].append(date)\n",
    "\n",
    "\n",
    "def obtain_duration_groups(raw_rows, valid_station_days):\n",
    "    # key: station\n",
    "    # value: (total duration, number of trips)\n",
    "    # then can compute avg by taking v[0]/v[1]\n",
    "    station_durations = {}\n",
    "    for row in rows:\n",
    "        station = row[\"Station\"]\n",
    "        if not station in station_durations.keys():\n",
    "            station_durations[station] = (0,0)\n",
    "            \n",
    "        total_duration = 0\n",
    "        for i in station_date_rides[(station, date)]:\n",
    "            total_duration += raw_rows[i][\"Duration\"]\n",
    "        \n",
    "        station_durations[station][0] += total_duration\n",
    "        station_durations[station][1] += len(station_date_rides[(station, date)])\n",
    "    \n",
    "    avg_station_durations = {k:(v[0]/v[1]) for k,v in station_durations.items()}\n",
    "    sort_station_durations = [(k,v) for k,v in station_durations.items()]\n",
    "    sort_station_durations.sort()\n",
    "    sort_station_durations_names = [k for (k,_) in sort_station_durations]\n",
    "    \n",
    "    nstations = len(sort_station_durations_names)\n",
    "    ngroups = 5\n",
    "    ratio = int(nstations/ngroups)\n",
    "    \n",
    "    groups = []\n",
    "    for i in range(ngroups):\n",
    "        groups.append(sort_station_durations_names[ratio*i:ratio*(i+1)])\n",
    "    groups[-1] += sort_station_durations_names[int(sort_station_durations_names/ngroups)*ngroups:]\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
